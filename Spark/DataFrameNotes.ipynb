{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK DATA FRAME DOCUMENTATION\n",
    "\n",
    "ELÄ°F CANSU YILDIZ   \n",
    "20.12.2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.dataframe import *\n",
    "import json\n",
    "\n",
    "spark = SparkSession\\\n",
    " .builder\\\n",
    " .appName(\"DataFrameOrnegi\")\\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[123456, Computer...|[[michael, armbru...|\n",
      "|[789012, Mechanic...|[[matei,, no-repl...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[345678, Theater ...|[[michael, armbru...|\n",
      "|[901234, Indoor R...|[[xiangrui, meng,...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+--------+--------+\n",
      "| column1| column2|\n",
      "+--------+--------+\n",
      "|value1_1|value2_2|\n",
      "|value2_1|value2_2|\n",
      "+--------+--------+\n",
      "\n",
      "+---------+\n",
      "|     data|\n",
      "+---------+\n",
      "|Spark SQL|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "df1.show()\n",
    "\n",
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = spark.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "df2.show()\n",
    "\n",
    "#in basic form\n",
    "df = spark.createDataFrame([('value1_1','value2_2'),('value2_1','value2_2')], ['column1','column2'])\n",
    "df = spark.createDataFrame([('value1_1','value2_2'),('value2_1','value2_2')], ('column1','column2'))\n",
    "df.show()\n",
    "\n",
    "df = spark.createDataFrame([('Spark SQL',)], ['data'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing some values and Using Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='123456', name='Computer Science')\n",
      "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n",
      "no-reply@berkeley.edu\n",
      "Row(department=Row(id='123456', name='Computer Science'), employees=[Row(firstName='michael', lastName='armbrust', email='no-reply@berkeley.edu', salary=100000), Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)])\n",
      "Row(department=Row(id='123456', name='Computer Science'), employees=[Row(firstName='michael', lastName='armbrust', email='no-reply@berkeley.edu', salary=100000), Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)])\n",
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[123456, Computer...|[[michael, armbru...|\n",
      "|[789012, Mechanic...|[[matei,, no-repl...|\n",
      "|[345678, Theater ...|[[michael, armbru...|\n",
      "|[901234, Indoor R...|[[xiangrui, meng,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(department1)\n",
    "print(employee2)\n",
    "print(departmentWithEmployees1.employees[0].email)\n",
    "print(departmentWithEmployees1)\n",
    "\n",
    "departmentWithEmployees = Row(\"department\", \"employees\")\n",
    "\n",
    "dptWithEmp = departmentWithEmployees(department1, [employee1,employee2])\n",
    "print(dptWithEmp)\n",
    "\n",
    "unionDF = df1.union(df2)   #unionDF = df1.unionAll(df2)\n",
    "unionDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using explode function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|              e_mail|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = unionDF.select(functions.explode(\"employees\").alias(\"e\"))\n",
    "explodeDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email as e_mail\", \"e.salary\")\n",
    "\n",
    "explodeDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Filter and Where Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "filterDF = \n",
      "\n",
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|              e_mail|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n",
      "\n",
      "filter with OR \n",
      "\n",
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|              e_mail|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n",
      "\n",
      "filterDF type =  <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "whereDf = \n",
      "\n",
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|              e_mail|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n",
      "\n",
      "nonNullDf = \n",
      "\n",
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|              e_mail|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "|       --| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|       --| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n",
      "\n",
      "filternonNullDf = \n",
      "\n",
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|              e_mail|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n",
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|              e_mail|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterDF = explodeDF.filter(explodeDF.firstName == \"xiangrui\").sort(explodeDF.lastName)\n",
    "print(\"\\nfilterDF = \\n\")\n",
    "filterDF.show()\n",
    "\n",
    "# Use `|` instead of `or`\n",
    "filterDF = explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "print(\"\\nfilter with OR \\n\")\n",
    "filterDF.show()\n",
    "print(\"\\nfilterDF type = \", type(filterDF))\n",
    "\n",
    "whereDF = explodeDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "print(\"\\nwhereDf = \\n\")\n",
    "whereDF.show()\n",
    "\n",
    "#Replace null values with -- using DataFrame Na function\n",
    "nonNullDF = explodeDF.fillna(\"--\")\n",
    "print(\"\\nnonNullDf = \\n\")\n",
    "nonNullDF.show()\n",
    "\n",
    "filterNonNullDF = explodeDF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"e_mail\")\n",
    "print(\"\\nfilternonNullDf = \\n\")\n",
    "filterNonNullDF.show()\n",
    "explodeDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|salary|max(salary)|\n",
      "+------+-----------+\n",
      "|120000|     120000|\n",
      "|140000|     140000|\n",
      "|100000|     100000|\n",
      "|160000|     160000|\n",
      "+------+-----------+\n",
      "\n",
      "+---------+--------+-------------------------+\n",
      "|firstName|lastName|count(DISTINCT firstName)|\n",
      "+---------+--------+-------------------------+\n",
      "|     null| wendell|                        0|\n",
      "|    matei|    null|                        1|\n",
      "| xiangrui|    meng|                        1|\n",
      "|  michael|armbrust|                        1|\n",
      "+---------+--------+-------------------------+\n",
      "\n",
      "root\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- count(DISTINCT firstName): long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupByDf = explodeDF.groupBy(\"salary\").max(\"salary\")\n",
    "groupByDf.show()\n",
    "\n",
    "countDistinctDF = explodeDF.select(\"firstName\", \"lastName\")\\\n",
    "  .groupBy(\"firstName\", \"lastName\")\\\n",
    "  .agg(countDistinct(\"firstName\"))\n",
    "  #.select(\"firstName\", \"lastName\", col(\"count(DISTINCT firstName)\").alias(\"countDistinct\"))\n",
    "countDistinctDF.show()\n",
    "\n",
    "countDistinctDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------+------+\n",
      "|     id|salary|\n",
      "+-------+------+\n",
      "|123456%|  1000|\n",
      "|456123%|  2000|\n",
      "|123456%|  3000|\n",
      "|456123%|  1500|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = Row(id=\"123456%\", salary=1000)\n",
    "example2= Row(id=\"456123%\", salary=2000)\n",
    "example3= Row(id=\"123456%\", salary=3000)\n",
    "example4= Row(id=\"456123%\", salary=1500)\n",
    "\n",
    "exampleSeq = [example,example2,example3,example4]\n",
    "exmpDF = spark.createDataFrame(exampleSeq)\n",
    "exmpDF.printSchema()\n",
    "exmpDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+\n",
      "|     id|salary|wage|\n",
      "+-------+------+----+\n",
      "|123456%|  1000|1000|\n",
      "|456123%|  2000|2000|\n",
      "|123456%|  3000|3000|\n",
      "|456123%|  1500|1500|\n",
      "+-------+------+----+\n",
      "\n",
      "+----+------+\n",
      "|  id|salary|\n",
      "+----+------+\n",
      "|1000|  1000|\n",
      "|2000|  2000|\n",
      "|3000|  3000|\n",
      "|1500|  1500|\n",
      "+----+------+\n",
      "\n",
      "+-------+----+\n",
      "|     id| max|\n",
      "+-------+----+\n",
      "|123456%|3000|\n",
      "|456123%|2000|\n",
      "+-------+----+\n",
      "\n",
      "+------+-----------+\n",
      "|salary|max(salary)|\n",
      "+------+-----------+\n",
      "|  1000|       1000|\n",
      "|  1500|       1500|\n",
      "|  3000|       3000|\n",
      "|  2000|       2000|\n",
      "+------+-----------+\n",
      "\n",
      "+------+----+\n",
      "|salary| max|\n",
      "+------+----+\n",
      "|  1000|1000|\n",
      "|  1500|1500|\n",
      "|  3000|3000|\n",
      "|  2000|2000|\n",
      "+------+----+\n",
      "\n",
      "+------+-----------+----+\n",
      "|salary|max(salary)|wage|\n",
      "+------+-----------+----+\n",
      "|  1000|       1000|1000|\n",
      "|  1500|       1500|1500|\n",
      "|  3000|       3000|3000|\n",
      "|  2000|       2000|2000|\n",
      "+------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exmpDF.withColumn(\"wage\",exmpDF[\"salary\"]).show()\n",
    "\n",
    "exmpDF.withColumn(\"id\", exmpDF[\"salary\"].cast(LongType())).show()\n",
    "\n",
    "exmpDF.groupBy(\"id\").agg(max(\"salary\").alias(\"max\")).show()\n",
    "\n",
    "exmpDF.groupBy(\"salary\").max().alias(\"max\").show()  #alias fonksiyonu bu Åekilde iÅe yaramadÄ±!!\n",
    "\n",
    "#in order to change the column name of aggregate function:\n",
    "exmpDF.groupBy(\"salary\").max().select(\"salary\", col(\"max(salary)\").alias(\"max\")).show()\n",
    "\n",
    "exmpDF.groupBy(\"salary\").max().withColumn(\"wage\",exmpDF[\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+\n",
      "|     id|salary|newColumn|\n",
      "+-------+------+---------+\n",
      "|123456%|  1000| 123456.0|\n",
      "|456123%|  2000| 456123.0|\n",
      "|123456%|  3000| 123456.0|\n",
      "|456123%|  1500| 456123.0|\n",
      "+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exmpDF.withColumn(\"newColumn\", expr(\"concat(substring(id,1,length(id)-1))\").cast(DoubleType())).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using length and concat and expr function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+\n",
      "|     id|salary|newColumn|\n",
      "+-------+------+---------+\n",
      "|123456%|  1000| 123456_%|\n",
      "|456123%|  2000| 456123_%|\n",
      "|123456%|  3000| 123456_%|\n",
      "|456123%|  1500| 456123_%|\n",
      "+-------+------+---------+\n",
      "\n",
      "+-------+------+------+\n",
      "|     id|salary|length|\n",
      "+-------+------+------+\n",
      "|123456%|  1000|     7|\n",
      "|456123%|  2000|     7|\n",
      "|123456%|  3000|     7|\n",
      "|456123%|  1500|     7|\n",
      "+-------+------+------+\n",
      "\n",
      "+------+-----------+\n",
      "| newID|avg(salary)|\n",
      "+------+-----------+\n",
      "|123456|     2000.0|\n",
      "|456123|     1750.0|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exmpDF.withColumn(\"newColumn\",concat(substring(exmpDF[\"id\"],1,6), lit(\"_\"), substring(exmpDF[\"id\"],7,7))).show() \n",
    "\n",
    "exmpDF.withColumn(\"length\", length(\"id\")).show() \n",
    "\n",
    "exmpDF2 = exmpDF.withColumn(\"newID\", expr(\"concat(substring(id,1,length(id)-1))\").cast(LongType()))\n",
    "exmpDF2 = exmpDF2.groupBy(\"newID\").avg(\"salary\")\n",
    "exmpDF2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123456%\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "results = json.loads(exmpDF.toJSON().first())\n",
    "for key in results:\n",
    "    print (results[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE FROM SAMPLE KAFKA DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- container: string (nullable = true)\n",
      " |-- cpu: string (nullable = true)\n",
      " |-- pids: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datum1 = Row(container = \"7ba51fd5c3ac\", cpu = \"0.03%\", pids = 2)\n",
    "datum2 = Row(container = \"e8ccbc91d6dd\", cpu = \"9.17%\", pids = 2)\n",
    "datum3 = Row(container = \"7ba51fd5c3ac\", cpu = \"15.07%\", pids = 2)\n",
    "datum4 = Row(container = \"e8ccbc91d6dd\", cpu = \"3.49%\", pids = 2)\n",
    "datum5 = Row(container = \"7ba51fd5c3ac\", cpu = \"4.79%\", pids = 2)\n",
    "datum6 = Row(container = \"7ba51fd5c3ac\", cpu = \"15.07%\", pids = 2)\n",
    "\n",
    "dataSequence = [datum1, datum2, datum3, datum4, datum5, datum6]\n",
    "df = spark.createDataFrame(dataSequence)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|   container|avg(newcpu)|\n",
      "+------------+-----------+\n",
      "|e8ccbc91d6dd|       6.33|\n",
      "|7ba51fd5c3ac|       8.74|\n",
      "+------------+-----------+\n",
      "\n",
      "root\n",
      " |-- container: string (nullable = true)\n",
      " |-- max(newcpu): double (nullable = true)\n",
      "\n",
      "+------------+-----------+\n",
      "|   container|max(newcpu)|\n",
      "+------------+-----------+\n",
      "|e8ccbc91d6dd|       9.17|\n",
      "|7ba51fd5c3ac|      15.07|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"newcpu\", expr(\"concat(substring(cpu,1,length(cpu)-1))\").cast(DoubleType())).groupBy(\"container\").avg(\"newcpu\").show()\n",
    "\n",
    "query = df\\\n",
    "    .withColumn(\"newcpu\", expr(\"concat(substring(cpu,1,length(cpu)-1))\").cast(DoubleType()))\\\n",
    "    .groupBy(\"container\")\\\n",
    "    .max('newcpu')\n",
    "\n",
    "query.printSchema()\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|maxcpu|   container|\n",
      "+------+------------+\n",
      "|  9.17|e8ccbc91d6dd|\n",
      "| 15.07|7ba51fd5c3ac|\n",
      "+------+------------+\n",
      "\n",
      "root\n",
      " |-- maxcpu: string (nullable = true)\n",
      " |-- container: string (nullable = true)\n",
      "\n",
      "+------+------------+--------------------+\n",
      "|maxcpu|   container|          jsonColumn|\n",
      "+------+------------+--------------------+\n",
      "|  9.17|e8ccbc91d6dd|{\"maxcpu\":\"9.17\",...|\n",
      "| 15.07|7ba51fd5c3ac|{\"maxcpu\":\"15.07\"...|\n",
      "+------+------------+--------------------+\n",
      "\n",
      "+-----------------+-----+\n",
      "|              avg|  max|\n",
      "+-----------------+-----+\n",
      "|7.936666666666667|15.07|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2 = query.select(col(\"max(newcpu)\").alias(\"maxcpu\").cast(StringType()), \"container\")\n",
    "\n",
    "query2.show()\n",
    "query2.printSchema()\n",
    "\n",
    "query2 = query2.withColumn(\"jsonColumn\", to_json(struct([query2[x] for x in query2.columns])))\n",
    "query2.show()\n",
    "\n",
    "query3 = df\\\n",
    "    .withColumn(\"newcpu\", expr(\"concat(substring(cpu,1,length(cpu)-1))\").cast(DoubleType()))\\\n",
    "    .agg(max(\"newcpu\"), avg(\"newcpu\"))\\\n",
    "    .select(col(\"avg(newcpu)\").alias(\"avg\"), col(\"max(newcpu)\").alias(\"max\"))\n",
    "query3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using join function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 :\n",
      "+------------+------+----+------+\n",
      "|   container|   cpu|pids|newcpu|\n",
      "+------------+------+----+------+\n",
      "|7ba51fd5c3ac| 0.03%|   2|  0.03|\n",
      "|e8ccbc91d6dd| 9.17%|   2|  9.17|\n",
      "|7ba51fd5c3ac|15.07%|   2| 15.07|\n",
      "|e8ccbc91d6dd| 3.49%|   2|  3.49|\n",
      "|7ba51fd5c3ac| 4.79%|   2|  4.79|\n",
      "|7ba51fd5c3ac|15.07%|   2| 15.07|\n",
      "+------------+------+----+------+\n",
      "\n",
      "query2:\n",
      "+------+------------+--------------------+\n",
      "|maxcpu|newcontainer|          jsonColumn|\n",
      "+------+------------+--------------------+\n",
      "|  9.17|e8ccbc91d6dd|{\"maxcpu\":9.17,\"n...|\n",
      "| 15.07|7ba51fd5c3ac|{\"maxcpu\":15.07,\"...|\n",
      "+------+------------+--------------------+\n",
      "\n",
      "using left_outer join:\n",
      "+------+--------------------+------------+\n",
      "|maxcpu|          jsonColumn|   container|\n",
      "+------+--------------------+------------+\n",
      "| 15.07|{\"maxcpu\":15.07,\"...|7ba51fd5c3ac|\n",
      "|  9.17|{\"maxcpu\":9.17,\"n...|e8ccbc91d6dd|\n",
      "+------+--------------------+------------+\n",
      "\n",
      "using inner join:\n",
      "+------+--------------------+------------+\n",
      "|maxcpu|          jsonColumn|   container|\n",
      "+------+--------------------+------------+\n",
      "| 15.07|{\"maxcpu\":15.07,\"...|7ba51fd5c3ac|\n",
      "|  9.17|{\"maxcpu\":9.17,\"n...|e8ccbc91d6dd|\n",
      "+------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.withColumn(\"newcpu\", expr(\"concat(substring(cpu,1,length(cpu)-1))\").cast(DoubleType())).groupBy(\"container\").avg(\"newcpu\").show()\n",
    "\n",
    "print(\"df2 :\")\n",
    "df2 = df.withColumn(\"newcpu\", expr(\"concat(substring(cpu,1,length(cpu)-1))\").cast(DoubleType()))\n",
    "#df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "print(\"query2:\")\n",
    "query2 = query.select(col(\"max(newcpu)\").alias(\"maxcpu\").cast(DoubleType()), col(\"container\").alias(\"newcontainer\"))\n",
    "query2 = query2.withColumn(\"jsonColumn\", to_json(struct([query2[x] for x in query2.columns])))\n",
    "#query2.printSchema()\n",
    "query2.show()\n",
    "\n",
    "print(\"using left_outer join:\")\n",
    "query3 = query2\\\n",
    "        .join(df2, (query2.newcontainer == df2.container) & (query2.maxcpu == df2.newcpu), 'left_outer' )\\\n",
    "        .dropDuplicates(['newcontainer','maxcpu'])\n",
    "query3.select(\"maxcpu\",\"jsonColumn\",\"container\").show()\n",
    "#query3 = query2.join(df2, df2[\"container\"] == query2[\"container\"] & df2[\"newcpu\"]==query2[\"maxcpu\"], 'outer')\n",
    "\n",
    "print(\"using inner join:\")\n",
    "query3 = query2\\\n",
    "        .join(df2, (query2.newcontainer == df2.container) & (query2.maxcpu == df2.newcpu), 'inner' )\\\n",
    "        .dropDuplicates(['newcontainer','maxcpu'])\n",
    "query3.select(\"maxcpu\",\"jsonColumn\",\"container\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using udf (user defined function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-----+\n",
      "| id|    name|age| word|\n",
      "+---+--------+---+-----+\n",
      "|  1|John Doe| 21|hello|\n",
      "+---+--------+---+-----+\n",
      "\n",
      "+-----------+------+\n",
      "|slenn(name)|plusli|\n",
      "+-----------+------+\n",
      "|          8|    22|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "find_length = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "dfExample = spark.createDataFrame([(1, \"John Doe\", 21, \"hello\")], (\"id\", \"name\", \"age\",\"word\"))\n",
    "dfExample.show()\n",
    "\n",
    "@udf('integer')\n",
    "# Input/output are both a single double value\n",
    "def plus_one(v):\n",
    "      return v+1 \n",
    "\n",
    "@udf('string')\n",
    "# Input/output are both a single double value\n",
    "def store_it(v):\n",
    "      return v \n",
    "\n",
    "dfExample.select(slen(\"name\").alias(\"slenn(name)\"), plus_one(\"age\").alias(\"plusli\")).show()\n",
    "dfExample = dfExample.withColumn(\"v2\", plus_one(\"age\")).withColumn(\"v3\",find_length(\"word\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|column1|column2|column3|\n",
      "+-------+-------+-------+\n",
      "|      2|      1|   Alex|\n",
      "|      1|      1| Starle|\n",
      "+-------+-------+-------+\n",
      "\n",
      "+-------+-------+-------+----+\n",
      "|column1|column2|column3|calc|\n",
      "+-------+-------+-------+----+\n",
      "|      2|      1|   Alex|   1|\n",
      "|      1|      1| Starle|null|\n",
      "+-------+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values = [(2, 1,\"Alex\"), (1,1,\"Starle\")]\n",
    "columns = [\"column1\", \"column2\", \"column3\"]\n",
    "df = spark.createDataFrame(values,columns)\n",
    "df.show()\n",
    "\n",
    "@udf('integer')\n",
    "def calc_dif(x,y):\n",
    "    if (x>y) and (x!=1):\n",
    "        return int(x)-int(y)     #or   calc_dif = udf(calc_dif, IntegerType())\n",
    "    \n",
    "dfNew = df.withColumn(\"calc\", lit(calc_dif(\"column1\",df[\"column2\"])))\n",
    "dfNew.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+--------+---+---+\n",
      "| id|    name|age|    word| v2| v3|\n",
      "+---+--------+---+--------+---+---+\n",
      "|  1|John Doe| 21|   hello| 22|  5|\n",
      "|  1|John Doe| 21|helloooo| 23|  8|\n",
      "+---+--------+---+--------+---+---+\n",
      "\n",
      "+-----+\n",
      "|myCol|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|   20|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfExample = spark.createDataFrame([(1, \"John Doe\", 21, \"hello\")], (\"id\", \"name\", \"age\",\"word\"))\n",
    "tmpDF = spark.createDataFrame([(1, \"John Doe\", 21, \"helloooo\", 23, 8)])\n",
    "dfExample = dfExample.union(tmpDF)\n",
    "dfExample.show()\n",
    "\n",
    "firstDF = spark.range(3).toDF(\"myCol\")\n",
    "newRow = spark.createDataFrame([[20]])\n",
    "appended = firstDF.union(newRow)\n",
    "appended.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read From JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- container: string (nullable = true)\n",
      " |-- cpu: string (nullable = true)\n",
      " |-- io: struct (nullable = true)\n",
      " |    |-- block: string (nullable = true)\n",
      " |    |-- network: string (nullable = true)\n",
      " |-- memory: struct (nullable = true)\n",
      " |    |-- percent: string (nullable = true)\n",
      " |    |-- raw: string (nullable = true)\n",
      " |-- pids: string (nullable = true)\n",
      "\n",
      "+------------+\n",
      "|   container|\n",
      "+------------+\n",
      "|7ba51fd5c3ac|\n",
      "|e8ccbc91d6dd|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF = spark.read.json(\"dosya.json\")\n",
    "jsonDF.printSchema()\n",
    "jsonDF.where(\"pids == 2\").select(\"container\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using expr, when, selectExpr, if in expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|column1|column2|column3|\n",
      "+-------+-------+-------+\n",
      "|      2|      0|   Alex|\n",
      "|      1|      2| Starle|\n",
      "+-------+-------+-------+\n",
      "\n",
      "+-------+-------+-------+----+\n",
      "|column1|column2|column3|calc|\n",
      "+-------+-------+-------+----+\n",
      "|      2|      0|   Alex|   2|\n",
      "|      1|      2| Starle|null|\n",
      "+-------+-------+-------+----+\n",
      "\n",
      "+-------+-------+-------+----+\n",
      "|column1|column2|column3|calc|\n",
      "+-------+-------+-------+----+\n",
      "|      2|      0|   Alex|   4|\n",
      "|      1|      2| Starle|   6|\n",
      "+-------+-------+-------+----+\n",
      "\n",
      "+-------+-------+-------+----+\n",
      "|column1|column2|column3|calc|\n",
      "+-------+-------+-------+----+\n",
      "|      2|      0|   Alex|   5|\n",
      "|      1|      2| Starle|   4|\n",
      "+-------+-------+-------+----+\n",
      "\n",
      "+---------------+------------+\n",
      "|length(column3)|abs(column1)|\n",
      "+---------------+------------+\n",
      "|              4|           2|\n",
      "|              6|           1|\n",
      "+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def calc_dif2(x,y):\n",
    "    when((x > y) & (x != 1), x - y)\"\"\"\n",
    "\n",
    "df = spark.createDataFrame([(2, 0,\"Alex\"), (1,2,\"Starle\")],[\"column1\", \"column2\", \"column3\"])\n",
    "df.show()\n",
    "\n",
    "dfNew = df.withColumn(\"calc\", when(df[\"column1\"]>df[\"column2\"], df[\"column1\"]-df[\"column2\"]))\n",
    "dfNew.show()\n",
    "\n",
    "dfNew = df.withColumn(\"calc\", expr(\"length(column3)\"))\n",
    "dfNew.show()\n",
    "\n",
    "dfNew = df.withColumn(\"calc\", expr(\"if (column2>0 , 4, if(column1 == 2, 5, 0))\"))\n",
    "dfNew.show()\n",
    "\n",
    "dfNew = df.selectExpr(\"length(column3)\", \"abs(column1)\")\n",
    "dfNew.show()\n",
    "\n",
    "new_column_1 = expr(\n",
    "    \"\"\"IF(fruit1 IS NULL OR fruit2 IS NULL, 3, IF(fruit1 = fruit2, 1, 0))\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using concat, expr func:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---------+\n",
      "|   s|  d|newColumn|\n",
      "+----+---+---------+\n",
      "|abcd|123|  abcd123|\n",
      "+----+---+---------+\n",
      "\n",
      "+-------+\n",
      "|  cnlds|\n",
      "+-------+\n",
      "|abcd123|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
    "df.withColumn(\"newColumn\", concat(df.s, df.d)).show()   #df.withColumn(\"newColumn2\", expr(\"concat(s, d)\")).show()\n",
    "df.select(expr(\"concat(s, d)\").alias(\"cnlds\")).show()   #df.select(concat(df.s, df.d).alias(\"cnlds\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|101| abc| 24|\n",
      "|102| cde| 24|\n",
      "|103| efg| 22|\n",
      "|104| ghi| 21|\n",
      "|105| ijk| 20|\n",
      "|106| klm| 19|\n",
      "|107| mno| 18|\n",
      "|108| pqr| 18|\n",
      "|109| rst| 26|\n",
      "|110| tuv| 27|\n",
      "|111| pqr| 18|\n",
      "|112| rst| 28|\n",
      "|113| tuv| 29|\n",
      "+---+----+---+\n",
      "\n",
      "+---+----+---+-----+\n",
      "| id|name|age|count|\n",
      "+---+----+---+-----+\n",
      "|107| mno| 18|    3|\n",
      "|108| pqr| 18|    3|\n",
      "|111| pqr| 18|    3|\n",
      "|106| klm| 19|    1|\n",
      "|105| ijk| 20|    1|\n",
      "|104| ghi| 21|    1|\n",
      "|103| efg| 22|    1|\n",
      "|102| cde| 24|    2|\n",
      "|101| abc| 24|    2|\n",
      "|109| rst| 26|    1|\n",
      "|110| tuv| 27|    1|\n",
      "|112| rst| 28|    1|\n",
      "|113| tuv| 29|    1|\n",
      "+---+----+---+-----+\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|avg(id)|          avg(age)|        avg(count)|\n",
      "+-------+------------------+------------------+\n",
      "|  107.0|22.615384615384617|1.6153846153846154|\n",
      "+-------+------------------+------------------+\n",
      "\n",
      "+------------------+\n",
      "|          avg(age)|\n",
      "+------------------+\n",
      "|22.615384615384617|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    " .builder\\\n",
    " .appName(\"DataFrameOrnegi\")\\\n",
    " .getOrCreate()\n",
    "\n",
    "name_list = [(101, 'abc', 24), (102, 'cde', 24), (103, 'efg', 22), (104, 'ghi', 21),\n",
    "             (105, 'ijk', 20), (106, 'klm', 19), (107, 'mno', 18), (108, 'pqr', 18),\n",
    "             (109, 'rst', 26), (110, 'tuv', 27), (111, 'pqr', 18), (112, 'rst', 28), (113, 'tuv', 29)]\n",
    "\n",
    "\n",
    "name_age_df = spark.createDataFrame(name_list, ['id', 'name', 'age'])\n",
    "age_w = Window.partitionBy(\"age\")\n",
    "name_age_df.show()\n",
    "\n",
    "name_age_count_df = name_age_df.withColumn(\"count\", F.count(\"id\").over(age_w)).orderBy(\"age\")\n",
    "name_age_count_df.show()\n",
    "\n",
    "name_age_count_df_avg = name_age_count_df.groupBy().avg()\n",
    "name_age_count_df_avg.show()\n",
    "\n",
    "name_age_count_df_avg = name_age_count_df.groupBy().agg({\"age\":\"mean\"})\n",
    "name_age_count_df_avg.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using to_json function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+\n",
      "|   id|name|age|salary|\n",
      "+-----+----+---+------+\n",
      "|10001|alex| 30| 75000|\n",
      "|10002| bob| 30| 80000|\n",
      "|10003| deb| 30| 80000|\n",
      "|10004|john| 30| 85000|\n",
      "|10005| sam| 30| 75000|\n",
      "+-----+----+---+------+\n",
      "\n",
      "+-----+----+---+------+--------------------+\n",
      "|   id|name|age|salary|             jsonCol|\n",
      "+-----+----+---+------+--------------------+\n",
      "|10001|alex| 30| 75000|{\"id\":\"10001\",\"na...|\n",
      "|10002| bob| 30| 80000|{\"id\":\"10002\",\"na...|\n",
      "|10003| deb| 30| 80000|{\"id\":\"10003\",\"na...|\n",
      "|10004|john| 30| 85000|{\"id\":\"10004\",\"na...|\n",
      "|10005| sam| 30| 75000|{\"id\":\"10005\",\"na...|\n",
      "+-----+----+---+------+--------------------+\n",
      "\n",
      "+---+----+--------------------------------------------------------+\n",
      "|age|name|jsonCol                                                 |\n",
      "+---+----+--------------------------------------------------------+\n",
      "|30 |alex|{\"id\":\"10001\",\"name\":\"alex\",\"age\":\"30\",\"salary\":\"75000\"}|\n",
      "|30 |bob |{\"id\":\"10002\",\"name\":\"bob\",\"age\":\"30\",\"salary\":\"80000\"} |\n",
      "|30 |deb |{\"id\":\"10003\",\"name\":\"deb\",\"age\":\"30\",\"salary\":\"80000\"} |\n",
      "|30 |john|{\"id\":\"10004\",\"name\":\"john\",\"age\":\"30\",\"salary\":\"85000\"}|\n",
      "|30 |sam |{\"id\":\"10005\",\"name\":\"sam\",\"age\":\"30\",\"salary\":\"75000\"} |\n",
      "+---+----+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"10001\",\"alex\",\"30\",\"75000\"),(\"10002\",\"bob\",\"30\",\"80000\"),(\"10003\",\"deb\",\"30\",\"80000\"),(\"10004\",\"john\",\"30\",\"85000\"),(\"10005\",\"sam\",\"30\",\"75000\")],[\"id\",\"name\",\"age\",\"salary\"])\n",
    "df.show()\n",
    "#df.withColumn(\"jsonCol\", to_json(struct([ when(col(x)!=\"  \",df[x]).otherwise(None) for x in df.columns]))).show()\n",
    "df = df.withColumn(\"jsonCol\", to_json(struct([when(F.col(x)!=\"  \",df[x]).otherwise(None).alias(x) for x in df.columns])))\n",
    "df.show()\n",
    "df.select(df.age.cast(\"string\"),\"name\",\"jsonCol\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using monotically_increasing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"idx\",monotonically_increasing_id().alias('id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Explode function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rules: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- conditions: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- action: string (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------+----+\n",
      "|conditions                                                                                                  |type|\n",
      "+------------------------------------------------------------------------------------------------------------+----+\n",
      "|[[CPU has been increased by 50 percent, maxLimit, 70], [CPU has been decreased by 50 percent, minLimit, 10]]|CPU |\n",
      "|[[RAM has been increased by 50 percent, maxLimit, 12], [RAM has been decreased by 50 percent, minLimit, 2]] |RAM |\n",
      "+------------------------------------------------------------------------------------------------------------+----+\n",
      "\n",
      "+------------------------------------+--------+-----+\n",
      "|action                              |name    |value|\n",
      "+------------------------------------+--------+-----+\n",
      "|CPU has been increased by 50 percent|maxLimit|70   |\n",
      "|CPU has been decreased by 50 percent|minLimit|10   |\n",
      "|RAM has been increased by 50 percent|maxLimit|12   |\n",
      "|RAM has been decreased by 50 percent|minLimit|2    |\n",
      "+------------------------------------+--------+-----+\n",
      "\n",
      "+----+------------------------------------+--------+-----+\n",
      "|type|action                              |name    |value|\n",
      "+----+------------------------------------+--------+-----+\n",
      "|CPU |CPU has been increased by 50 percent|maxLimit|70   |\n",
      "|CPU |CPU has been decreased by 50 percent|minLimit|10   |\n",
      "|RAM |RAM has been increased by 50 percent|maxLimit|12   |\n",
      "|RAM |RAM has been decreased by 50 percent|minLimit|2    |\n",
      "+----+------------------------------------+--------+-----+\n",
      "\n",
      "+------------------------------------+\n",
      "|action                              |\n",
      "+------------------------------------+\n",
      "|CPU has been increased by 50 percent|\n",
      "|CPU has been decreased by 50 percent|\n",
      "|RAM has been increased by 50 percent|\n",
      "+------------------------------------+\n",
      "\n",
      "+----+---+------+---------+\n",
      "|name|id |salary|tmp_value|\n",
      "+----+---+------+---------+\n",
      "|aa  |100|1500  |11       |\n",
      "|bb  |101|1850  |18       |\n",
      "|cc  |102|1375  |3        |\n",
      "+----+---+------+---------+\n",
      "\n",
      "+------------------------------------+-----+---------+\n",
      "|action                              |value|tmp_value|\n",
      "+------------------------------------+-----+---------+\n",
      "|CPU has been decreased by 50 percent|10   |11       |\n",
      "|RAM has been decreased by 50 percent|2    |11       |\n",
      "+------------------------------------+-----+---------+\n",
      "\n",
      "+----+------------------------------------+--------+-----+\n",
      "|type|action                              |name    |value|\n",
      "+----+------------------------------------+--------+-----+\n",
      "|CPU |CPU has been increased by 50 percent|maxLimit|70   |\n",
      "|CPU |CPU has been decreased by 50 percent|minLimit|10   |\n",
      "|RAM |RAM has been increased by 50 percent|maxLimit|12   |\n",
      "|RAM |RAM has been decreased by 50 percent|minLimit|2    |\n",
      "+----+------------------------------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#json sample = {\"rules\": [{\"type\": \"CPU\",\"conditions\": [{\"name\": \"maxLimit\",\"value\": \"70\",\"action\": \"CPU has been increased by 50 percent\"}, {\"name\": \"minLimit\",\"value\": \"10\",\"action\": \"CPU has been decreased by 50 percent\"}]}, {\"type\": \"RAM\",\"conditions\": [{\"name\": \"maxLimit\",\t\"value\": \"12\",\"action\": \"RAM has been increased by 50 percent\"}, {\"name\": \"minLimit\",\"value\": \"2\",\"action\": \"RAM has been decreased by 50 percent\"}]}]}\n",
    "df = spark.read.json('rules.json')\n",
    "df.printSchema()\n",
    "\"\"\"df = df.withColumn(\"data\", explode(\"rules\")).selectExpr(\"data.*\")\n",
    "df.show()\"\"\"\n",
    "\n",
    "df2 = df.select(explode(\"rules\").alias(\"r\"))\n",
    "df2 = df2.selectExpr(\"r.*\")\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df3 = df2.select(explode(\"conditions\").alias(\"c\")).selectExpr(\"c.*\")\n",
    "df3.show(truncate=False)\n",
    "\n",
    "df4 = df2.withColumn(\"c\",explode(\"conditions\")).selectExpr(\"type\",\"c.*\")\n",
    "df4.show(truncate=False)\n",
    "\n",
    "result = df3.select(\"action\").where(\"value>=10\")\n",
    "result.show(truncate=False)\n",
    "\n",
    "tmpDF = spark.createDataFrame([(\"aa\",100,1500,11),(\"bb\",101,1850,18),(\"cc\",102,1375,3)],[\"name\",\"id\",\"salary\",\"tmp_value\"])\n",
    "tmpDF.show(truncate=False)\n",
    "\n",
    "#conc = df3.select(\"action\").where(\"tmpDF.value > 10\")\n",
    "conc = df4.join(tmpDF, ((tmpDF.tmp_value>df4.value)) & (tmpDF.salary==1500)).select(\"action\",\"value\",\"tmp_value\")\n",
    "conc.show(truncate=False)\n",
    "\n",
    "#SUMMARY:\n",
    "df.select(explode(\"rules\").alias(\"r\"))\\\n",
    "    .selectExpr(\"r.*\")\\\n",
    "    .withColumn(\"c\",explode(\"conditions\"))\\\n",
    "    .selectExpr(\"type\",\"c.*\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using regexp_extract function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+-----+--------------------+----+\n",
      "|           eventTime|   container|              memory|  cpu|                  io|pids|\n",
      "+--------------------+------------+--------------------+-----+--------------------+----+\n",
      "|2018-12-28 13:55:...|99f4e1ab4b98|[99.98%, 255.9MiB...|8.09%|[1.55GB / 1.81GB,...|   2|\n",
      "|2018-12-28 13:55:...|99f4e1ab4b98|[99.94%, 255.8MiB...|3.50%|[1.47GB / 1.73GB,...|   2|\n",
      "|2018-12-28 13:55:...|99f4e1ab4b98|[99.98%, 256MiB /...|8.58%|[8GB / 1.34GB, 3....|   2|\n",
      "|2018-12-25 15:55:...|99f4e1ab4b98|[99.98%, 255.9MiB...|8.09%|[1.55GB / 8GB, 0k...|   2|\n",
      "|2018-12-25 15:55:...|99f4e1ab4b98|[99.98%, 255.9MiB...|8.09%|[7GB / 3GB, 6kB /...|   2|\n",
      "+--------------------+------------+--------------------+-----+--------------------+----+\n",
      "\n",
      "+------------+----+-------+------+-------+-------+----+--------------------------+----------+\n",
      "|container   |cpu |mem_usg|netw_i|block_i|block_o|pids|eventTime                 |network   |\n",
      "+------------+----+-------+------+-------+-------+----+--------------------------+----------+\n",
      "|99f4e1ab4b98|8.09|99.98% |0     |1.55   | 1.81  |2   |2018-12-28 13:55:53.521274|0- 2      |\n",
      "|99f4e1ab4b98|3.5 |99.94% |3.33kB|1.47   | 1.73  |2   |2018-12-28 13:55:53.021234|3.33kB-.33|\n",
      "|99f4e1ab4b98|8.58|99.98% |3.12kB|1.34   | 1.34  |2   |2018-12-28 13:55:48.521993|3.12kB-.12|\n",
      "|99f4e1ab4b98|8.09|99.98% |0     |1.55   |       |2   |2018-12-25 15:55:53.521274|0- 4.15   |\n",
      "|99f4e1ab4b98|8.09|99.98% |6     |       |       |2   |2018-12-25 15:55:53.521274|6- 4.15   |\n",
      "+------------+----+-------+------+-------+-------+----+--------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json sample = [{\"eventTime\":\"2018-12-28 13:55:53.521274\",\"container\":\"99f4e1ab4b98\",\"memory\":{\"raw\":\"255.9MiB / 256MiB\",\"percent\":\"99.98%\"},\"cpu\":\"8.09%\",\"io\":{\"network\":\"0kB / 2B\",\"block\":\"1.55GB / 1.81GB\"},\"pids\":\"2\"}, {\"eventTime\":\"2018-12-28 13:55:53.021234\",\"container\":\"99f4e1ab4b98\",\"memory\":{\"raw\":\"255.8MiB / 256MiB\",\"percent\":\"99.94%\"},\"cpu\":\"3.50%\",\"io\":{\"network\":\"3.33kB / 0B\",\"block\":\"1.47GB / 1.73GB\"},\"pids\":\"2\"}, {\"eventTime\":\"2018-12-28 13:55:48.521993\",\"container\":\"99f4e1ab4b98\",\"memory\":{\"raw\":\"256MiB / 256MiB\",\"percent\":\"99.98%\"},\"cpu\":\"8.58%\",\"io\":{\"network\":\"3.12kB / 0B\",\"block\":\"8GB / 1.34GB\"},\"pids\":\"2\"}, {\"eventTime\":\"2018-12-25 15:55:53.521274\",\"container\":\"99f4e1ab4b98\",\"memory\":{\"raw\":\"255.9MiB / 256MiB\",\"percent\":\"99.98%\"},\"cpu\":\"8.09%\",\"io\":{\"network\":\"0kB / 4.15B\",\"block\":\"1.55GB / 8GB\"},\"pids\":\"2\"}, {\"eventTime\":\"2018-12-25 15:55:53.521274\",\"container\":\"99f4e1ab4b98\",\"memory\":{\"raw\":\"255.9MiB / 256MiB\",\"percent\":\"99.98%\"},\"cpu\":\"8.09%\",\"io\":{\"network\":\"6kB / 4.15B\",\"block\":\"7GB / 3GB\"},\"pids\":\"2\"}]\n",
    "rawdeneme = spark.read.json(\"exampleFile.json\")\n",
    "rawdeneme = rawdeneme.alias(\"json_value\")\\\n",
    "        .selectExpr(\n",
    "        \"json_value.eventTime\",\n",
    "        \"cast (json_value.container as string)\",\n",
    "        \"json_value.memory\",\n",
    "        \"cast (json_value.cpu as string)\",\n",
    "        \"json_value.io\",\n",
    "        \"cast (json_value.pids as integer)\"\n",
    "    )\n",
    "rawdeneme.show()\n",
    "\n",
    "deneme = rawdeneme\\\n",
    "    .withColumn(\"cpu\", expr(\"substring(cpu,1,length(cpu)-1)\").cast(DoubleType()))\\\n",
    "    .withColumn(\"memory_per\", col(\"memory.percent\"))\\\n",
    "    .withColumn(\"netw_i\", regexp_extract(\"io.network\", '(\\d+).(\\d+)(\\w+)|(\\d+)', 0))\\\n",
    "    .withColumn(\"netw_o\", regexp_extract(\"io.network\", '(.)(\\d+).(\\d+)|(.)(\\d+)', 0))\\\n",
    "    .withColumn(\"block_i\", regexp_extract(\"io.block\",'(\\d+).(\\d+)|(/d+)', 0))\\\n",
    "    .withColumn(\"block_o\", regexp_extract(\"io.block\",'(.)(\\d+).(\\d+)', 0))\\\n",
    "    .withColumn(\"network\",concat(col(\"netw_i\"),lit(\"-\"),col(\"netw_o\")))\\\n",
    "    .select(\"container\",\"cpu\",col(\"memory_per\").alias(\"mem_usg\"),\"netw_i\",\"block_i\",\"block_o\",\"pids\",\"eventTime\",\"network\")\n",
    "deneme.show(truncate=False)\n",
    "\n",
    "#https://stackoverflow.com/questions/46410887/pyspark-string-matching-to-create-new-column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: \n",
    "\n",
    "https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "\n",
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GÃ¶z atÄ±labilir: \n",
    "\n",
    "https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\n",
    "\n",
    "https://stackoverflow.com/questions/38549/what-is-the-difference-between-inner-join-and-outer-join?rq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /home/ubuntu/hadoop/logs/hadoop-ubuntu-namenode-master.out\n",
      "localhost: starting datanode, logging to /home/ubuntu/hadoop/logs/hadoop-ubuntu-datanode-master.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /home/ubuntu/hadoop/logs/hadoop-ubuntu-secondarynamenode-master.out\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /home/ubuntu/hadoop/logs/yarn-ubuntu-resourcemanager-master.out\n",
      "localhost: starting nodemanager, logging to /home/ubuntu/hadoop/logs/yarn-ubuntu-nodemanager-master.out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "start-all.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
