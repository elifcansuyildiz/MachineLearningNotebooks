{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.0 pyspark-shell'\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki kod çalışıyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"StructuredKafkaWordCount\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Create DataSet representing the stream of input lines from kafka\n",
    "    lines = spark\\\n",
    "        .readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "        .option(\"subscribe\", \"Stats\")\\\n",
    "        .load()\\\n",
    "        .selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "    # Split the lines into words\n",
    "    words = lines\\\n",
    "        .select(\n",
    "        # explode turns each item in an array into a separate row\n",
    "        explode(\n",
    "            split(lines.value, ' ')\n",
    "        ).alias('word') \n",
    "    )\n",
    "\n",
    "    words.printSchema()\n",
    "\n",
    "    # Generate running word count\n",
    "    wordCounts = words\\\n",
    "        .groupBy('word').count()\n",
    "\n",
    "    wordCounts = wordCounts.withColumn(\"jsonColumn\", to_json(struct([wordCounts[x] for x in wordCounts.columns])))\n",
    "\n",
    "    wordCounts.printSchema()\n",
    "\n",
    "    # Start running the query that prints the running counts to the console\n",
    "query = wordCounts\\\n",
    "        .selectExpr(\"CAST(jsonColumn AS string) as value\")\\\n",
    "        .writeStream\\\n",
    "        .format('kafka')\\\n",
    "        .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "        .option('topic', 'Action')\\\n",
    "        .option('checkpointLocation', '/home/ubuntu/spark/chkpoint/')\\\n",
    "        .outputMode('complete')\\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki kod append modda hata vermekte, complete yapılırsa düzeliyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"StructuredKafkaWordCount\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Create DataSet representing the stream of input lines from kafka\n",
    "    lines = spark\\\n",
    "        .readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "        .option(\"subscribe\", \"Stats\")\\\n",
    "        .load()\\\n",
    "        .selectExpr(\"CAST(value AS STRING)\",\"CAST(timestamp AS timestamp)\")\n",
    "\n",
    "    lines.printSchema()\n",
    "\n",
    "    # Split the lines into words\n",
    "    words = lines\\\n",
    "        .withWatermark(\"timestamp\",\"10 seconds\")\\\n",
    "        .select(\n",
    "        # explode turns each item in an array into a separate row\n",
    "        explode(\n",
    "            split(lines.value, ' ')\n",
    "        ).alias('word') , \"timestamp\"\n",
    "    )\n",
    "\n",
    "    words.printSchema()\n",
    "\n",
    "    # Generate running word count\n",
    "    wordCounts = words\\\n",
    "        .withWatermark(\"timestamp\",\"10 seconds\")\\\n",
    "        .groupBy('word').count()\n",
    "\n",
    "    wordCounts = wordCounts.withColumn(\"jsonColumn\", to_json(struct([wordCounts[x] for x in wordCounts.columns])))\n",
    "\n",
    "    wordCounts.printSchema()\n",
    "\n",
    "    # Start running the query that prints the running counts to the console\n",
    "query = wordCounts\\\n",
    "        .selectExpr(\"CAST(jsonColumn AS string) as value\")\\\n",
    "        .writeStream\\\n",
    "        .format('kafka')\\\n",
    "        .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "        .option('topic', 'Action')\\\n",
    "        .option('checkpointLocation', '/home/ubuntu/spark/chkpoint/')\\\n",
    "        .outputMode('append')\\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter'de Bash Kullanımı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "/home/ubuntu/kafka_2.12-1.1.0/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n",
    "/home/ubuntu/kafka_2.12-1.1.0/bin/kafka-topics.sh --list --zookeeper localhost:2181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### İncelenebilecek Kaynaklar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-kafka-spark-structured-streaming\n",
    "https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-jupyter-notebook-use-external-packages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
