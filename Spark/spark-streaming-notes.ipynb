{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __SPARK STREAMING DOCUMENTATION__\n",
    "\n",
    "ELİF CANSU YILDIZ  \n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu döküman Kafka Topic'ten veri alınması, başka Kafka topic'e yazdırılması, akan veri üzerindeki sorguları ve detayları açıklamaktadır. \n",
    "\n",
    "This documentation generally includes some explanation about how to retrieve data from Kafka Topic, process them and write back to Kafka Topic in JSON format. And also gives some information about details of commands and structure of Spark streaming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOT__ : Bu notebook'taki açıklamalar en aşağıda belirtilen LAST VERSION kodundaki detayları açıklamaktadır. Kodun son hali resourcemanager projesinde güncellenmiş olabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Requirements\n",
    "\n",
    "Apache Kafka  \n",
    "Apache Spark\n",
    "\n",
    "Aşağıdaki kodlar için Kafka Topic'e veri akışı olması gerekmektedir.\n",
    "\n",
    "Not: \n",
    "Aşağıdaki site ile Kafka arka planda servis olarak çalıştırabilir. Kafka yüklüyse sadece 3. ve 4. adımların takip edilmesi yeterlidir.\n",
    "https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-ubuntu-18-04\n",
    "\n",
    "## System Properties\n",
    "- Ubuntu 16-04\n",
    "- Apache Kafka_2.12-1.1.0\n",
    "- Apache Spark-2.4.0-bin-hadoop2.7\n",
    "\n",
    "Aşağıdaki kodlar  \n",
    "./bin/spark-submit deneme.py --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0  \n",
    "komutu ile çalıştırılmalıdır.\n",
    "\n",
    "Not: jar dosyası hatası verirse :  \n",
    "spark-sql-kafka-0-10_2.11-2.4.0.jar  \n",
    "spark-streaming-kafka-0-10_2.11-2.4.0.jar  \n",
    "spark-streaming-kafka-0-10-assembly_2.11-2.4.0.jar  \n",
    "dosyaları eksik olabilir. Sondaki versiyon numaraları önemlidir. Kullanılan kafka, scala ve spark versiyonlarına bakılarak uygun jar'lar seçilmelidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Explanation\n",
    "\n",
    "Bu kısımda kodun güncel halinin açıklamaları verilmektedir. Açıklamalar 4. versiyon kodu üzerinden yapılmıştır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    " .builder\\\n",
    " .appName(\"StructuredKafkaOrnegi\")\\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming'de kafka'dan alınan veri default bir şema ile gelmektedir. Bu şema şu şekildedir.\n",
    "\n",
    "root  \n",
    " |-- key: binary (nullable = true)  \n",
    " |-- value: binary (nullable = true)  \n",
    " |-- topic: string (nullable = true)  \n",
    " |-- partition: integer (nullable = true)  \n",
    " |-- offset: long (nullable = true)  \n",
    " |-- timestamp: timestamp (nullable = true)  \n",
    " |-- timestampType: integer (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"Stats\")\\\n",
    "    .load()\\\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selectExpr komutu ile şemayı sadece value değeri olacak şekilde düzenleyebiliyoruz.   \n",
    "root  \n",
    " |-- value: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka Topic'ten gelen verimiz şu şekildedir:   \n",
    "{\"eventTime\":\"2018-12-20 16:11:54.776866\",\"container\":\"7fae3d29f912\",\"memory\":{\"raw\":\"255.9MiB / 256MiB\",\"percent\":\"99.96%\"},\"cpu\":\"0.00%\",\"io\":{\"network\":\"286B / 0B\",\"block\":\"73.7kB / 150MB\"},\"pids\":\"2\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka'dan JSON olarak veri alınacağı için gelen verinin şeması oluşturulmaktadır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"eventTime\", TimestampType(), True),\n",
    "    StructField(\"container\", StringType(), True),\n",
    "    StructField(\"memory\", StructType([\n",
    "        StructField(\"raw\", StringType(), True),\n",
    "        StructField(\"percent\", StringType(), True)])),\n",
    "    StructField(\"cpu\", StringType(), True),\n",
    "    StructField(\"io\", StructType([\n",
    "        StructField(\"network\", StringType(), True),\n",
    "        StructField(\"block\", StringType(), True)])),\n",
    "    StructField(\"pids\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki kod ile oluşturulacak şema:  \n",
    "root   \n",
    " |-- eventTime: timestamp (nullable = true)  \n",
    " |-- container: string (nullable = true)  \n",
    " |-- memory: struct (nullable = true)  \n",
    " |    |-- raw: string (nullable = true)  \n",
    " |    |-- percent: string (nullable = true)  \n",
    " |-- cpu: string (nullable = true)  \n",
    " |-- io: struct (nullable = true)  \n",
    " |    |-- network: string (nullable = true)  \n",
    " |    |-- block: string (nullable = true)  \n",
    " |-- pids: integer (nullable = true)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"),schema).alias(\"json_value\"))\\\n",
    "    .selectExpr(\n",
    "        \"json_value.eventTime\",\n",
    "        \"cast (json_value.container as string)\",\n",
    "        \"json_value.memory\",\n",
    "        \"cast (json_value.cpu as string)\",\n",
    "        \"json_value.io\",\n",
    "        \"cast (json_value.pids as integer)\"\n",
    ")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki kodda;\n",
    "\n",
    "WithColumn ile yeni bir kolona istenilen ifade kaydedilebilmektedir. Mesela concat ve substring fonksiyonları ile CPU verisinin sonundaki % işareti kaldırılmış ve newcpu isimli yeni kolona yazdırılmıştır.  \n",
    "root   \n",
    " |-- eventTime: timestamp (nullable = true)  \n",
    " |-- container: string (nullable = true)  \n",
    " |-- memory: struct (nullable = true)  \n",
    " |    |-- raw: string (nullable = true)   \n",
    " |    |-- percent: string (nullable = true)   \n",
    " |-- cpu: string (nullable = true)   \n",
    " |-- io: struct (nullable = true)   \n",
    " |    |-- network: string (nullable = true)   \n",
    " |    |-- block: string (nullable = true)  \n",
    " |-- pids: integer (nullable = true)   \n",
    " |-- newcpu: double (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\\\n",
    "    .withColumn(\"newcpu\", expr(\"concat(substring(cpu,1,length(cpu)-1))\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki kodda;\n",
    "\n",
    "Windowing yapılmıştır. (window duration=10second, slide duration=5second)\n",
    "\n",
    "Watermark ile window aralığına geç kalan veriler için belirli bir gecikme süresi konulmuş, bu gecikme süresi ile geç gelen veriler de o window aralığında değerlendirilmiştir. \n",
    "\n",
    "groupBy fonksiyonundan sonra mutlaka bir agregate fonksiyonu (max, min, avg, sum, count) kullanılmalıdır. GroupBy fonksiyonu direkt writestream fonksiyonu veya show fonksiyonu ile çalıştırılamaz.\n",
    "\n",
    "agg fonksiyonu ile birden çok aggregate fonksiyonu çalıştırılabilir.\n",
    "groupBy'den sonra direkt max vb. fonksiyon da çalıştırılabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df\\\n",
    "    .withWatermark(\"eventTime\",\"10 seconds\")\\\n",
    "    .groupBy(\"container\", window(\"eventTime\",\"10 seconds\", \"5 seconds\"))\\\n",
    "    .agg(max('newcpu'),avg('newcpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max, avg gibi fonksiyonlar direkt ekstra kolon oluşturmakta ve \"max(cpu)\", \"avg(cpu)\" gibi kolon ismi vermekte, bu kolon isimleri direkt alias fonksiyonu ile düzeltilememektedir. Bunun için select fonksiyonu içinde col(\"max(cpu)\").alias(\"newcpu\") denilmelidir. \n",
    "\n",
    "groupBy'dan sonra yapılan agregate fonksiyonları ile şemadaki eleman sayısı azalmaktadır. \n",
    "select ile sadece window start time, end time, container(groupBy'ı buna göre yaptığımız için) ve max, avg hesaplamalarına erişilebilmektedir.    \n",
    "Aşağıdaki kodda Select sonucu şema:   \n",
    "root    \n",
    " |-- start: string (nullable = true)    \n",
    " |-- end: string (nullable = true)    \n",
    " |-- newcontainer: string (nullable = true)   \n",
    " |-- maxcpu: double (nullable = true)    \n",
    " |-- avgcpu: double (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res.select(df_res.window.start.cast(\"string\").alias(\"start\"),\n",
    "                       df_res.window.end.cast(\"string\").alias(\"end\"),\n",
    "                       col(\"container\").alias(\"newcontainer\"),\n",
    "                       col(\"max(newcpu)\").alias(\"maxcpu\"),\n",
    "                       col(\"avg(newcpu)\").alias(\"avgcpu\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki kodda;\n",
    "\n",
    "join fonksiyonu kullanılarak eski data frame'in eventTime değerine erişilmiştir. Select ile istenilen şema oluşturulmuştur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res\\\n",
    "    .join(df, (df_res.newcontainer == df.container) & (df_res.maxcpu == df.newcpu), 'inner' )\\\n",
    "    .select(\"container\",\"eventTime\",\"maxcpu\",\"avgcpu\",\"start\",\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "root   \n",
    " |-- container: string (nullable = true)   \n",
    " |-- eventTime: timestamp (nullable = true)   \n",
    " |-- maxcpu: double (nullable = true)   \n",
    " |-- avgcpu: double (nullable = true)   \n",
    " |-- start: string (nullable = true)   \n",
    " |-- end: string (nullable = true)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki kodda dataframe to_json fonksiyonu ile JSON formatına dönüştürülmüştür ve jsonColumn olarak şemaya eklenmiştir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res.withColumn(\"jsonColumn\", to_json(struct([df_res[x] for x in df_res.columns])))\n",
    "df_res.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "root   \n",
    " |-- container: string (nullable = true)   \n",
    " |-- eventTime: timestamp (nullable = true)   \n",
    " |-- maxcpu: double (nullable = true)  \n",
    " |-- avgcpu: double (nullable = true)   \n",
    " |-- start: string (nullable = true)   \n",
    " |-- end: string (nullable = true)   \n",
    " |-- jsonColumn: string (nullable = true)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not: Streaming'de sistemin bilgilerini (offset, topic gibi) tutabilmesi için checkpointLocation dosya yolu oluşturulmalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res\\\n",
    "    .selectExpr(\"cast (jsonColumn as string) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "    .option('topic', 'Action')\\\n",
    "    .option('checkpointLocation', '/home/ubuntu/spark/chkpoint/')\\\n",
    "    .outputMode('append')\\\n",
    "    .start()\n",
    "\n",
    "df_res.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Önemli Notlar:__   \n",
    "3 streaming output mode bulunmaktadır: \n",
    "- Append\n",
    "- Complete\n",
    "- Update\n",
    "\n",
    "Append ve complete modları denenmiştir.\n",
    "\n",
    "Complete modda geçmişteki veriler de baz alınarak sorgulamalar çalıştırılmaktadır. Ancak Append'de sadece en son gelenler işlenmektedir.\n",
    "\n",
    "join fonksiyonu complete modda çalışmamaktadır. Watermark kullanmadan da çalışmamaktadır. Hata vermektedir.\n",
    "\n",
    "Windowing hem complete hem de append modda çalışmaktadır ancak her bir çalıştırılışta complete mod geçmişteki tüm verileri tekrardan işlemektedir. Windowing için WaterMark şart değildir.\n",
    "\n",
    "Spark streaming'in micro batch özelliğinden kaynaklı akan veri de gruplar halinde aktarılmakta, bu yüzden farklı zamanlarda kod çalıştırılırsa geçmişten kalan veriler sonraki çalıştırmada gözükmektedir. Ancak spark streaming sürekli çalıştırılması gerektiğinden bu özelliğin herhangi bir duruma etkisi yoktur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASIC FORM\n",
    "\n",
    "Kafka topic'ten alınan veri Kafka topic'e yazdırılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    " .builder\\\n",
    " .appName(\"StructuredKafkaOrnegi\")\\\n",
    " .getOrCreate()\n",
    "\n",
    "df = spark\\\n",
    " .readStream\\\n",
    " .format(\"kafka\")\\\n",
    " .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    " .option(\"subscribe\", \"AA\")\\\n",
    " .load()\\\n",
    " .selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "query = df\\\n",
    " .selectExpr(\"CAST(value AS STRING)\")\\\n",
    " .writeStream\\\n",
    " .format('kafka')\\\n",
    " .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    " .option('topic', 'BB')\\\n",
    " .option('checkpointLocation', '/home/ubuntu/spark/chkpoint/')\\\n",
    " .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST VERSION\n",
    "Kafka topic'ten verinin JSON formatında alınabilmesi için şema oluşturuldu ve 'value' olarak Kafka topic'ten JSON alındı. Başka topic'e aynı JSON 'value' olarak kaydedildi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    " .builder\\\n",
    " .appName(\"StructuredKafkaOrnegi\")\\\n",
    " .getOrCreate()\n",
    "\n",
    "userSchema = StructType().add(\"value\", \"string\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"surname\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"AA\")\\\n",
    "    .load()\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"json_value\"))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# df = df.select(\"json_value\").where(length(\"json_value.name\") > 4)\n",
    "df = df.where(\"json_value.name == 'cansu'\")\n",
    "\n",
    "query = df\\\n",
    "    .selectExpr(\"to_json(struct(json_value.*)) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "    .option('topic', 'BB')\\\n",
    "    .option('checkpointLocation', '/home/ubuntu/spark/chkpoint/')\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECOND VERSION\n",
    "First versiondan farklı olarak JSON data üzerinde sorgulama yapıldı ve sorgu sonucu JSON datanın istenilen kolonu JSON olarak yeni Kafka topic'e yazdırıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSchema = StructType().add(\"value\", \"string\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"surname\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "    StructField(\"ktime\", TimestampType(), True),\n",
    "    StructField(\"container\", StringType(), True),\n",
    "    StructField(\"memory\", StructType([\n",
    "        StructField(\"raw\", StringType(), True),\n",
    "        StructField(\"percent\", StringType(), True)])),\n",
    "    StructField(\"cpu\", StringType(), True),\n",
    "    StructField(\"io\", StructType([\n",
    "        StructField(\"network\", StringType(), True),\n",
    "        StructField(\"block\", StringType(), True)])),\n",
    "    StructField(\"pids\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"AA\")\\\n",
    "    .option(\"subscribe\", \"Stats\")\\\n",
    "    .load()\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"json_value\"))\n",
    "    .select(from_json(col(\"value\").cast(\"string\"),schema).alias(\"json_value\"))\\\n",
    "    .selectExpr(\n",
    "        \"json_value.ktime\",\n",
    "        \"cast (json_value.container as string)\",\n",
    "        \"json_value.memory\",\n",
    "        \"cast (json_value.cpu as integer)\",\n",
    "        \"json_value.io\",\n",
    "        \"cast (json_value.pids as integer)\"\n",
    "    )\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "#df = df.select(\"json_value\")\n",
    "# df = df.select(\"json_value\").where(length(\"json_value.name\") > 4)\n",
    "df = df.where(\"json_value.name == 'cansu'\")\n",
    "#df = df.where(\"json_value.name == 'cansu'\")\n",
    "#df = df.where(\"json_value.pids == '2'\")\n",
    "\n",
    "df = df.filter(df[\"pids\"] == 2)\n",
    "#df = df.where(\"least(json_value.pids)\")\n",
    "\n",
    "\"\"\"query = df\\\n",
    "    .selectExpr(\"cast (memory as string) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "    .option('topic', 'Action')\\\n",
    "    .option('checkpointLocation', '/home/ubuntu/spark/chkpoint/')\\\n",
    "    .start()\"\"\"\n",
    "\n",
    "\n",
    "query = df\\\n",
    "    .selectExpr(\"to_json(struct(json_value.*)) AS value\")\\\n",
    "    .selectExpr(\"to_json(struct(io.network)) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "    .option('topic', 'BB')\\\n",
    "    .option('topic', 'Action')\\\n",
    "    .option('checkpointLocation', '/home/ubuntu/spark/chkpoint/')\\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki sorgular alternatif olarak kullanılabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.select(\"json_value\")\n",
    "# df = df.select(\"json_value\").where(length(\"json_value.name\") > 4)\n",
    "#df = df.where(\"json_value.name == 'cansu'\")\n",
    "#df = df.where(\"json_value.pids == '2'\")\n",
    "\n",
    "#df = df.filter(df[\"pids\"] == 2)\n",
    "#df = df.filter(\"pids == 3\")\n",
    "\n",
    "#df = df.groupBy(\"container\").agg(sum(\"pids\"),count(\"pids\"))\n",
    "\n",
    "#newdf = df.groupBy(\"container\").avg(\"cpu\")\n",
    "#windowedAvgSignalDF = df \\\n",
    "#    .groupBy(window(\"cpu\", \"5 minute\")) \\\n",
    "#    .count()\n",
    "\n",
    "#df = df.where(\"least(json_value.pids)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIRD VERSION \n",
    "\n",
    "JSON data is taken from Kafka topic and max CPU is written to Kafka Topic as JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    " .builder\\\n",
    " .appName(\"StructuredKafkaOrnegi\")\\\n",
    " .getOrCreate()\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ktime\", TimestampType(), True),\n",
    "    StructField(\"container\", StringType(), True),\n",
    "    StructField(\"memory\", StructType([\n",
    "        StructField(\"raw\", StringType(), True),\n",
    "        StructField(\"percent\", StringType(), True)])),\n",
    "    StructField(\"cpu\", StringType(), True),\n",
    "    StructField(\"io\", StructType([\n",
    "        StructField(\"network\", StringType(), True),\n",
    "        StructField(\"block\", StringType(), True)])),\n",
    "    StructField(\"pids\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"Stats\")\\\n",
    "    .load()\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"),schema).alias(\"json_value\"))\\\n",
    "    .selectExpr(\n",
    "        \"json_value.ktime\",\n",
    "        \"cast (json_value.container as string)\",\n",
    "        \"json_value.memory\",\n",
    "        \"cast (json_value.cpu as string)\",\n",
    "        \"json_value.io\",\n",
    "        \"cast (json_value.pids as integer)\"\n",
    "    )\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "query = df\\\n",
    "    .withColumn(\"newcpu\", expr(\"concat(substring(cpu,1,length(cpu)-1))\").cast(DoubleType()))\\\n",
    "    .groupBy(\"container\")\\\n",
    "    .agg(max('newcpu'),avg('newcpu'))\\\n",
    "    .select(\"container\",col(\"max(newcpu)\").alias(\"maxcpu\"),col(\"avg(newcpu)\").alias(\"avgcpu\").cast(DoubleType()))\n",
    "query.printSchema()\n",
    "\n",
    "query = query.withColumn(\"jsonColumn\", to_json(struct([query[x] for x in query.columns])))\n",
    "\n",
    "query.printSchema()\n",
    "\n",
    "query = query\\\n",
    "    .selectExpr(\"cast (jsonColumn as string) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "    .option('topic', 'Action')\\\n",
    "    .option('checkpointLocation', '/home/ubuntu/spark/chkpoint/')\\\n",
    "    .outputMode('complete')\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOURTH VERSION:\n",
    "\n",
    "Stats isimli Kafka topicten JSON olarak gelen verinin 10 saniyelik window periyodlarında maximum ve ortalama CPU değerleri hesaplanmaktadır.   \n",
    "\n",
    "Çıktı olarak;    \n",
    "- container id  \n",
    "- event time   \n",
    "- maximum CPU   \n",
    "- average CPU   \n",
    "- window start time   \n",
    "- window end time   \n",
    "\n",
    "değerleri JSON olarak Action isimli topic'e yazdırılmaktadır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession\\\n",
    " .builder\\\n",
    " .appName(\"StructuredKafkaOrnegi\")\\\n",
    " .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"eventTime\", TimestampType(), True),\n",
    "    StructField(\"container\", StringType(), True),\n",
    "    StructField(\"memory\", StructType([\n",
    "        StructField(\"raw\", StringType(), True),\n",
    "        StructField(\"percent\", StringType(), True)])),\n",
    "    StructField(\"cpu\", StringType(), True),\n",
    "    StructField(\"io\", StructType([\n",
    "        StructField(\"network\", StringType(), True),\n",
    "        StructField(\"block\", StringType(), True)])),\n",
    "    StructField(\"pids\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"Stats\")\\\n",
    "    .load()\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"),schema).alias(\"json_value\"))\\\n",
    "    .selectExpr(\n",
    "        \"json_value.eventTime\",\n",
    "        \"cast (json_value.container as string)\",\n",
    "        \"json_value.memory\",\n",
    "        \"cast (json_value.cpu as string)\",\n",
    "        \"json_value.io\",\n",
    "        \"cast (json_value.pids as integer)\"\n",
    "    )\n",
    "\n",
    "df = df\\\n",
    "    .withColumn(\"newcpu\", expr(\"concat(substring(cpu,1,length(cpu)-1))\").cast(DoubleType()))\n",
    "\n",
    "df_res = df\\\n",
    "    .withWatermark(\"eventTime\",\"10 seconds\")\\\n",
    "    .groupBy(\"container\", window(\"eventTime\",\"10 seconds\", \"5 seconds\"))\\\n",
    "    .agg(max('newcpu'),avg('newcpu'))\n",
    "\n",
    "df_res = df_res.select(df_res.window.start.cast(\"string\").alias(\"start\"),\n",
    "                       df_res.window.end.cast(\"string\").alias(\"end\"),\n",
    "                       col(\"container\").alias(\"newcontainer\"),\n",
    "                       col(\"max(newcpu)\").alias(\"maxcpu\"),\n",
    "                       col(\"avg(newcpu)\").alias(\"avgcpu\").cast(DoubleType()))\n",
    "\n",
    "df_res = df_res\\\n",
    "    .join(df, (df_res.newcontainer == df.container) & (df_res.maxcpu == df.newcpu), 'inner' )\\\n",
    "    .select(\"container\",\"eventTime\",\"maxcpu\",\"avgcpu\",\"start\",\"end\")\n",
    "\n",
    "df_res = df_res.withColumn(\"jsonColumn\", to_json(struct([df_res[x] for x in df_res.columns])))\n",
    "\n",
    "df_res.printSchema()\n",
    "\n",
    "df_res = df_res\\\n",
    "    .selectExpr(\"cast (jsonColumn as string) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "    .option('topic', 'Action')\\\n",
    "    .option('checkpointLocation', '/home/ubuntu/spark/chkpoint/')\\\n",
    "    .outputMode('append')\\\n",
    "    .start()\n",
    "\n",
    "df_res.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOT:__\n",
    "join fonksiyonu kullanıldıktan sonra aynı window'da aynı container için tek bir max değeri olamamaktadır. Çünkü verilerin eventTime'ları farklıdır. container ve max değerlerini baz alarak dropDuplicates fonksiyonu çalıştırılmak istendiğinde streaming'ten kaynaklı dropDuplicates fonksiyonu WaterMark gereksinimi duymaktadır. O kısımda hatalarla karşılaşılmış ve dropDuplicates henüz eklenmemiştir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAST VERSION\n",
    "\n",
    "Bu versiyonda groupBy ile tüm kolonlar çekildi, ardından agg fonksiyonu olarak max kullanıldı. Sonrasında filter fonksiyonu ile CPU değerlerinin değerlendirilebilmesi için rules.json dosyasındaki max ve min değerlerine göre action üretildi ve yeni kolon olarak eklendi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__:  Windowing için groupBy fonksiyonu gereklidir yani groupBy fonksiyonu içinde kullanılabilir, groupBy'dan sonra da GroupedData için agg fonksiyonu (max, min, avg, sum, count gibi) kullanılması gereklidir. Ardından filter, withColumn, select gibi fonksiyonlar kullanılabilir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import sys,os\n",
    "sys.path.insert(0, os.path.realpath(os.path.join(os.path.dirname(os.path.abspath(__file__)),'/home/ubuntu/Desktop/CANSU/resourcemanager-master')))\n",
    "from utils.jsonUtils.jsonParser import jsonParser\n",
    "\n",
    "spark = SparkSession\\\n",
    " .builder\\\n",
    " .appName(\"StructuredKafkaOrnegi\")\\\n",
    " .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"eventTime\", TimestampType(), True),\n",
    "    StructField(\"container\", StringType(), True),\n",
    "    StructField(\"memory\", StructType([\n",
    "        StructField(\"raw\", StringType(), True),\n",
    "        StructField(\"percent\", StringType(), True)])),\n",
    "    StructField(\"cpu\", StringType(), True),\n",
    "    StructField(\"io\", StructType([\n",
    "        StructField(\"network\", StringType(), True),\n",
    "        StructField(\"block\", StringType(), True)])),\n",
    "    StructField(\"pids\", StringType(), True)\n",
    "])\n",
    "\n",
    "filePath = os.path.join(os.path.dirname(os.path.abspath(__file__)),'/home/ubuntu/Desktop/CANSU/resourcemanager-master/config/rules.json')\n",
    "filePath = os.path.abspath(os.path.realpath(filePath))\n",
    "jsonPO = jsonParser(filePath)\n",
    "jsonPO.readJSONfromFile()\n",
    "jsonPO.reloadJsonfromFile()\n",
    "#print(jsonPO.jsonDict[\"rules\"][0][\"CPU\"][\"min\"])\n",
    "\n",
    "rawdf = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"Stats\")\\\n",
    "    .load()\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"),schema).alias(\"json_value\"))\\\n",
    "    .selectExpr(\n",
    "        \"json_value.eventTime\",\n",
    "        \"cast (json_value.container as string)\",\n",
    "        \"json_value.memory\",\n",
    "        \"cast (json_value.cpu as string)\",\n",
    "        \"json_value.io\",\n",
    "        \"cast (json_value.pids as integer)\"\n",
    "    )\n",
    "\n",
    "df = rawdf\\\n",
    "    .withColumn(\"cpu\", expr(\"substring(cpu,1,length(cpu)-1)\").cast(FloatType()))\\\n",
    "    .withColumn(\"ram\", expr(\"substring(memory.percent,1,length(memory.percent)-1)\"))\\\n",
    "    .withColumn(\"netw_i\", regexp_extract(\"io.network\", '(\\d+).(\\d+)|(\\d+)', 0))\\\n",
    "    .withColumn(\"block_i\", regexp_extract(\"io.block\",'(\\d+).(\\d+)|(\\d+)', 0))\\\n",
    "    .select(\"container\", \"cpu\", \"ram\", \"netw_i\", \"block_i\", \"pids\", \"eventTime\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df_cpu = df\\\n",
    "    .withWatermark(\"eventTime\", \"10 seconds\")\\\n",
    "    .dropDuplicates([\"eventTime\",\"container\",\"cpu\"])\\\n",
    "    .groupBy(df.container, df.cpu, df.ram, df.netw_i, df.block_i, df.eventTime, window(\"eventTime\", jsonPO.jsonDict[\"rules\"][0][\"CPU\"][\"windowDur\"], jsonPO.jsonDict[\"rules\"][0][\"CPU\"][\"slideDur\"]))\\\n",
    "    .agg(max(\"cpu\"))\\\n",
    "    .filter((df.cpu < float(jsonPO.jsonDict[\"rules\"][0][\"CPU\"][\"min\"])) | (df.cpu > float(jsonPO.jsonDict[\"rules\"][0][\"CPU\"][\"max\"])))\\\n",
    "    .withColumn(\"action_cpu\", when( df.cpu < float(jsonPO.jsonDict[\"rules\"][0][\"CPU\"][\"min\"]) , jsonPO.jsonDict[\"rules\"][0][\"CPU\"][\"minaction\"]).otherwise(jsonPO.jsonDict[\"rules\"][0][\"CPU\"][\"maxaction\"]) )\\\n",
    "    .select(df.container, df.eventTime, df.netw_i, df.block_i, df.ram, df.cpu, \"action_cpu\")\n",
    "\n",
    "df_cpu = df_cpu.withColumn(\"jsonColumn\", to_json(struct([df_cpu[x] for x in df_cpu.columns])))\n",
    "\n",
    "df_cpu = df_cpu\\\n",
    "    .selectExpr(\"cast (jsonColumn as string) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\\\n",
    "    .option('topic', 'Action')\\\n",
    "    .option('checkpointLocation', '/home/ubuntu/spark/chkpoint/')\\\n",
    "    .outputMode('append')\\\n",
    "    .start()\n",
    "\n",
    "df_cpu.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code References: \n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html\n",
    "\n",
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
